# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fg-CJPJhOsCviqb23HZqJjHTlGwTaFHk
"""

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.model_selection import train_test_split, GridSearchCV

from xgboost import XGBRegressor
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import joblib
from geopy.distance import geodesic


RANDOM_STATE = 1


def train_model(csv_file: str, model_file: str):
    # Read the CSV file
    df = pd.read_csv(csv_file)

    # Define features and target
    target = "price_sq"
    features = [
        "lat",
        "lon",
        "area",
        "floor",
        "kitchen_area",
        "balconies",
        "renovation",
        "is_apartment",
        "rooms",
        "ceiling_height",
        "house_floors",
        "lifts",
        "freight_lifts",
        "time_on_foot_to_subway",
        "build_year",
        "bathroom_type",
        "house_wall_type",
    ]
    numeric_features = [
        "lat",
        "lon",
        "area",
        "floor",
        "kitchen_area",
        "rooms",
        "ceiling_height",
        "house_floors",
        "lifts",
        "freight_lifts",
        "time_on_foot_to_subway",
        "build_year",
    ]
    categorical_features = [
        "renovation",
        "balconies",
        "bathroom_type",
        "house_wall_type",
    ]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="mean")),
            ("scaler", StandardScaler()),
        ],
        verbose=True,
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ],
        verbose=True,
    )

    preprocessor = ColumnTransformer(
        n_jobs=-1,
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
    )

    # Define the model pipeline
    model = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            (
                "regressor",
                XGBRegressor(
                    objective="reg:squarederror",
                    random_state=RANDOM_STATE,
                    verbosity=2,
                    max_depth=0,
                ),
            ),
        ],
        verbose=True,
    )

    X = df[features]
    y = df[target]
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.1, random_state=RANDOM_STATE
    )

    # model = joblib.load(model_file)
    # Train the model
    model.fit(X_train, y_train)

    # Save the model
    joblib.dump(model, model_file)

    print(f"Model trained and saved to {model_file}")
    # Predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Print evaluation metrics
    print("Model Evaluation:")
    print(f"R-squared: {r2:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")


train_model("kazan.csv", "kazan_model_xgb.pkl")
def clear_data():
    df = pd.read_csv("cian_25.11.24.csv")

    df = df[df["city"] == "Казань"]
    df = df[[
        "price_sq",
            "lat",
            "lon",
            "area",
            "floor",
            "kitchen_area",
            "balconies",
            "renovation",
            "is_apartment",
            "rooms",
            "ceiling_height",
            "house_floors",
            "lifts",
            "freight_lifts",
            "time_on_foot_to_subway",
            "build_year",
            "bathroom_type",
            "house_wall_type",
        ]]
        # Preprocess the data
    numeric_features = [
        "lat",
        "lon",
        "area",
        "floor",
        "kitchen_area",
        "rooms",
        "ceiling_height",
        "house_floors",
        "lifts",
        "freight_lifts",
        "time_on_foot_to_subway",
        "build_year",
    ]
    # Replace commas with dots for all numeric features
    for feature in numeric_features:
        df[feature] = df[feature].replace({",": "."}, regex=True).astype(float)
        df[feature] = pd.to_numeric(
            df[feature], errors="coerce"
        )  # Convert to float and handle errors
    df.to_csv("kazan.csv")

# clear_data()